Machine Learning Course

Notes

!! Indepentent Variable -> Feature
!! Dependent Variable -> Label

Problem Solving Process

1 - Identify the independent and dependent variables
  # Features are categories of data points that affect the
  value of a label
2 - Assemble a set of data related to the problem you are trying
    to solve
  # Datasets almost always cleanup or formatting
3 - Decide on the type of output you are Predicting
  # Regression used with continuous values, classification used
  with discrete values
4 - Based on type of output, pick an algorithm that will determine
    a correlation between your 'features' and 'labels'
  # Many different algorithms exist, each with pros and cons
5 - Use model generated by algorithm to make a prediction
  # Models relate the value of 'features' to the value of 'labels'

* Common outputs from ML Algorithm

Classification: The value of our labels belong to a discrete set.
Ex: Predicting a value that belongs to a determined set, a finite pool
# Based on how many hours a student studied for an exam, did they
    PASS or FAIL -> [PASS, FAIL]
# Based on the content of this email, is it SPAM or
    NOT SPAM -> [SPAM, NOT SPAM]
# Based on where a football player shoots from, are they likely
    to SCORE or NOT SCORE -> [SCORE, NOT SCORE]

KNN -> K-Nearest Neighbor (birds of a feather flock together)

Regression: The value of our labels belong to a continuous set.
Ex: Predicting any value in between two other, an interval
# Based on the year, make and model of a car, what is its 
    value? -> $0 to $50K
# Based on an individual's daily calorie intake and minutes 
    spent exercising, what is their weight? -> 80lb to 400lb
# Basend on the height of this pine tree, what is its 
    age? -> 0 to 500 years


||||||||||||||||||||||||||||||||
-------- KNN Discussion --------
||||||||||||||||||||||||||||||||

*-*Topic
How K-Nearest Neighbor Works

For this project, the algorithm will be built like this
1st Iteraction: With one independent variable

Objective: Which bucket will a ball go into if dropped at 300px?

1 -> Drop a ball a bunch of times all around the board, record which bucket it
goes into.
# Collect data of for comparision cases.

2 -> For each observation, subtract drop point from <target drop point> (in this
case 300px), take absolute values
# The subtraction has the objective of keeping track of how similar or dissimilar
each exemple data is compared to our target. The smallest, the simillar, the gratest
the dissimilar

3-> Sort the results from least to greatest
# Generate a list orded from the most simillar to dissimilar

4-> Look at the 'k' top records. What was the most common bucket?
# 'k' comes from the algorithm name and is the deciding factor for deciding
the results returned by the algorithm. There are many strategies to determine 'k'

5-> Whichever bucket came up most frequently is the one ours will probably go into

*-*Topic
Perfecting the algorithm

There will always be a optimization to be done to the algorithm. If the results
are not in accord to expected, it is up to the dev to think about the missing
pieces or interactions between the features and labels, that will be converted in
better predictions

* Our prediction was bad!
There are a couple topics that can be approached to better the results

1- Adjust the parameters of the analysis
# Maybe the testing point or predition point has some overlooked flaws or
missing info.

2- Add more features to explain the analysis
# Maybe the data opon which the algorithm will be making the predictions is not
'complete' a better understanding of the problem will uncover more labels, solve
missconceptions or remove wrongly added independent variables

3- Change the prediction point
# Maybe a static prediction point or missimputed data for the predition is messing
with the analysis

4- Accept that maybe there isn't a good correlation
#- Maybe the correlation between the independent and dependent variables were not
thouht straght or forced. A revision of the problem and the collected data will
provide more reliable correlations.

*! But before trying to fix or analyse all the above, K is a important variable for KNN

*-*Topic
Finding an ideal K

1- Record a bunch of data points
# Generate or collect the problem's data

2- Split that data into a 'training' set and a 'test' set
# The training subset should be bigger than the testing set.

3- For each 'test' record run KNN using the 'training' data
# Each time the algorithm is ran, it can be refactored, refined

4- Does the result of KNN is equal to the 'test' record bucket.
# Since it will be compared to real samples coleted from the problem 'data set'
it will demonstrate if changes applied to the algorithm is affecting for the
better or worse.

For each run of the algorithm and it's changes, the result will be compared to one
of the testing data set, recording if the 'prediction was correct or not.



